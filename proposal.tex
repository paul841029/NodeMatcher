\documentclass[12pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumitem}
\begin{document}
    \center{
{\bf CS 703}\\
Project Proposal\\
Paul Luh    
}
\begin{description}
    \item[Problem Statement]\text{}\\
    This project aims to implement a system that can synthesize \texttt{XPath} query for \texttt{DOM} data extraction from \texttt{HTML} pages. Given a \texttt{HTML} page and a set of annotated \texttt{DOM} nodes on the page, the system will automatically learn \texttt{XPath} query that can extract \texttt{DOM} nodes from different \texttt{HTML} pages but consistent with the patterns found in the user-provided examples.   
    
    From the perspective of information extraction, the set of user-provided \texttt{DOM} nodes can be considered as \textit{mentions} for a single \textit{entity type}. For example, if the \textit{entity type} is \texttt{President of the United States}, the set of user-provided examples can be tree nodes of \{\texttt{Bill Clinton}, \texttt{Barack Obama}, \texttt{Donald Trump}\}. 
    
    As the project progresses, I will answer the following questions throughout my investigation.
    \begin{enumerate}[label=(\alph*)]
        \item How efficient is the synthesis algorithm proposed in the reference papers \cite{Anton2005XPathWrapperIB}? What are the bottlenecks? 
        \item Are there any limitations in terms of expressiveness when using \texttt{XPath} query to extract information for real-world web pages? Do we need to make any simplicity assumption about the page structure so that \texttt{XPath} can perform competitively?
        \item How many examples are needed to have acceptable performances from the algorithms in the reference paper? Can we simply choose the examples arbitrarily or are there any patterns that we need to follow? 
    \end{enumerate}
       
    \item[Motivation]\text{}\\
    
    As more data are available on the Internet, collecting data from semi-structured web pages becomes an integral process when obtaining training data for machine learning applications, sources for knowledge base construction, and pieces of evidence for business analysis. However, although many web pages have similar structure and the information of interest typically shares similar patterns in those web pages, the development of web scrapers is still considered a labor-intensive activity since one need to (1) design the schema in which the extractor will populate (2) observe the patterns in the web page structure that can be utilized when writing web scrapers (3) develop the web scrapers and handle corner cases (4) update the web scrapper when the website provider changes the layout. Therefore, there are always great interests from the research community in developing a mechanism to automate the process of web data extraction \cite{dalvi2011automatic}. This process is typically coined in the data mining community as \textit{wrapper induction}.
    
    \item[Key Components]\text{}\\
    \begin{description}
        \item[Document Parser]\text{}\\
        The document parser will parse the \texttt{HTML} pages into in-memory representation on which the synthesis engine can execute. The parser will build on top of out-of-box \texttt{HTML/XML} parser from existing \texttt{Python} libraries. However, I expect minor postprocessing steps on the parse tree so the synthesis algorithm can operate properly.
        \item[Annotator]\text{}\\
        The annotator will map input annotations to the corresponding parts in the in-memory representation.
        \item[Synthesis Engine]\text{}\\
        A \texttt{Python} implementation of the graph-based synthesis algorithm from \cite{Anton2005XPathWrapperIB}.
        \item[Output Finalizer]\text{}\\
        Since the output of the synthesis engine will be a graph program, the output finalizer will translate the original program into \texttt{XPath}.  
    \end{description}
    \item[Experiments]\text{}\\
    I plan to conduct the following two lines of experiments.
    \begin{enumerate}[label=(\alph*)]
        \item Data extraction from Wikipedia infobox. Since infoboxes on Wikipedia pages have very similar structures and the presentation is relatively compact, this experiment is designed to demonstrate the engine's abilities to handle homogenous format from data sources. 
        \item Attribute extract from product pages from \texttt{Amazon}. This experiment is designed to stress test the engine if the webpages have more content and may contain noisy information.
    \end{enumerate}
I will measure the performance following the metrics typically used in information retrieval application namely \texttt{Accuracy, Recall, F1-Score}. The access of ground-truth is not clear at this point; I will figure this out when the project progresses. In the worst case, I will annotate them manually.
    \item[Milestones]\text{}\\
    \begin{description}
        
        \item[Deliverable 1] Parser that can parse the document into in-memory data structures. I need to fully understand the algorithm in the paper first so I know what representations are suitable for the synthesis algorithm.
        
        \item[Deliverable 2] Synthesis engine that can operate on the in-memory data structures given user-provided annotations.

        \item[Deliverable 3] Experiments for testing the synthesis engine.
    \end{description}

\item[Appendix: Progress Report]\text{}\\


	\begin{description}
		\item[Implementation Plan]\text{}\\
		\begin{enumerate}
			\item (completed) Understand the first algorithm in the paper [1].
			\item (completed) Design a data structure that this algorithm can be performed. (It turned out to be an augmented graph data structure)
			\item (completed) Crawled Wikipedia pages from the internet. (I crawled the infobox section of the U.S. President's Wikipedia page) 
			\item Crawled product pages from Amazon.com.
			\item (work in progress) Implement a compiler that can transform the raw representation into the data structure designed above.
			\item On top of the data structure, implement the algorithms investigated in bullet point 1. 
		\end{enumerate}
	\item[Experimentation Plan]\text{}\\
	\begin{enumerate}
		\item Try extracting the information about the political party of each president using the program learned by the algorithm. Will test the performance with different number of examples.
		\item Try extracting the price information from several product pages from Amazon.
	\end{enumerate}
	\end{description}
\end{description}
\nocite{Le2014FlashExtractAF} 
\nocite{yaghmazadeh2018automated}
\bibliographystyle{plain}
\bibliography{papers}

\end{document}